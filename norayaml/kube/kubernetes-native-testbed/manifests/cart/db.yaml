# Generated by below command
#   helm2 repo add pingcap https://charts.pingcap.org/
#   helm2 repo update
#   helm2 fetch --version v1.0.6 pingcap/tidb-cluster
#   helm2 template --name cart-db --namespace cart --set tidb.replicas=0 --set pd.storageClassName=rook-ceph-block --set tikv.storageClassName=rook-ceph-block --set monitor.create=false --set enableConfigMapRollout=false tidb-cluster-v1.0.6.tgz > db.yaml
#   add namespace cart to each metadata
---
# Source: tidb-cluster/templates/pd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-pd
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: pd
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start pd containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
        echo "entering debug mode."
        tail -f /dev/null
    fi
    
    # Use HOSTNAME if POD_NAME is unset for backward compatibility.
    POD_NAME=${POD_NAME:-$HOSTNAME}
    # the general form of variable PEER_SERVICE_NAME is: "<clusterName>-pd-peer"
    cluster_name=`echo ${PEER_SERVICE_NAME} | sed 's/-pd-peer//'`
    domain="${POD_NAME}.${PEER_SERVICE_NAME}.${NAMESPACE}.svc"
    discovery_url="${cluster_name}-discovery.${NAMESPACE}.svc:10261"
    encoded_domain_url=`echo ${domain}:2380 | base64 | tr "\n" " " | sed "s/ //g"`
    
    elapseTime=0
    period=1
    threshold=30
    while true; do
        sleep ${period}
        elapseTime=$(( elapseTime+period ))
    
        if [[ ${elapseTime} -ge ${threshold} ]]
        then
            echo "waiting for pd cluster ready timeout" >&2
            exit 1
        fi
    
        if nslookup ${domain} 2>/dev/null
        then
            echo "nslookup domain ${domain}.svc success"
            break
        else
            echo "nslookup domain ${domain} failed" >&2
        fi
    done
    
    ARGS="--data-dir=/var/lib/pd \
    --name=${POD_NAME} \
    --peer-urls=http://0.0.0.0:2380 \
    --advertise-peer-urls=http://${domain}:2380 \
    --client-urls=http://0.0.0.0:2379 \
    --advertise-client-urls=http://${domain}:2379 \
    --config=/etc/pd/pd.toml \
    "
    
    if [[ -f /var/lib/pd/join ]]
    then
        # The content of the join file is:
        #   demo-pd-0=http://demo-pd-0.demo-pd-peer.demo.svc:2380,demo-pd-1=http://demo-pd-1.demo-pd-peer.demo.svc:2380
        # The --join args must be:
        #   --join=http://demo-pd-0.demo-pd-peer.demo.svc:2380,http://demo-pd-1.demo-pd-peer.demo.svc:2380
        join=`cat /var/lib/pd/join | tr "," "\n" | awk -F'=' '{print $2}' | tr "\n" ","`
        join=${join%,}
        ARGS="${ARGS} --join=${join}"
    elif [[ ! -d /var/lib/pd/member/wal ]]
    then
        until result=$(wget -qO- -T 3 http://${discovery_url}/new/${encoded_domain_url} 2>/dev/null); do
            echo "waiting for discovery service to return start args ..."
            sleep $((RANDOM % 5))
        done
        ARGS="${ARGS}${result}"
    fi
    
    echo "starting pd-server ..."
    sleep $((RANDOM % 10))
    echo "/pd-server ${ARGS}"
    exec /pd-server ${ARGS}
    
  config-file: |-
    [log]
    level = "info"
    [replication]
    location-labels = ["region", "zone", "rack", "host"]
    

---
# Source: tidb-cluster/templates/tidb-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-tidb
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start tidb containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
        echo "entering debug mode."
        tail -f /dev/null
    fi
    
    ARGS="--store=tikv \
    --host=0.0.0.0 \
    --path=${CLUSTER_NAME}-pd:2379 \
    --config=/etc/tidb/tidb.toml
    "
    
    if [[ X${BINLOG_ENABLED:-} == Xtrue ]]
    then
        ARGS="${ARGS} --enable-binlog=true"
    fi
    
    SLOW_LOG_FILE=${SLOW_LOG_FILE:-""}
    if [[ ! -z "${SLOW_LOG_FILE}" ]]
    then
        ARGS="${ARGS} --log-slow-query=${SLOW_LOG_FILE:-}"
    fi
    
    echo "start tidb-server ..."
    echo "/tidb-server ${ARGS}"
    exec /tidb-server ${ARGS}
    
  config-file: |-
    [log]
    level = "info"
    

---
# Source: tidb-cluster/templates/tikv-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cart-db-tikv
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tikv
    helm.sh/chart: tidb-cluster-v1.0.6
data:
  startup-script: |-
    #!/bin/sh
    
    # This script is used to start tikv containers in kubernetes cluster
    
    # Use DownwardAPIVolumeFiles to store informations of the cluster:
    # https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api
    #
    #   runmode="normal/debug"
    #
    
    set -uo pipefail
    
    
    
    ANNOTATIONS="/etc/podinfo/annotations"
    
    if [[ ! -f "${ANNOTATIONS}" ]]
    then
        echo "${ANNOTATIONS} does't exist, exiting."
        exit 1
    fi
    source ${ANNOTATIONS} 2>/dev/null
    
    runmode=${runmode:-normal}
    if [[ X${runmode} == Xdebug ]]
    then
    	echo "entering debug mode."
    	tail -f /dev/null
    fi
    
    # Use HOSTNAME if POD_NAME is unset for backward compatibility.
    POD_NAME=${POD_NAME:-$HOSTNAME}
    ARGS="--pd=${CLUSTER_NAME}-pd:2379 \
    --advertise-addr=${POD_NAME}.${HEADLESS_SERVICE_NAME}.${NAMESPACE}.svc:20160 \
    --addr=0.0.0.0:20160 \
    --status-addr=0.0.0.0:20180 \
    --data-dir=/var/lib/tikv \
    --capacity=${CAPACITY} \
    --config=/etc/tikv/tikv.toml
    "
    
    echo "starting tikv-server ..."
    echo "/tikv-server ${ARGS}"
    exec /tikv-server ${ARGS}
    
  config-file: |-
    log-level = "info"
    

---
# Source: tidb-cluster/templates/discovery-rbac.yaml

kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
rules:
- apiGroups: ["pingcap.com"]
  resources: ["tidbclusters"]
  resourceNames: [cart-db]
  verbs: ["get"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
subjects:
- kind: ServiceAccount
  name: cart-db-discovery
roleRef:
  kind: Role
  name: cart-db-discovery
  apiGroup: rbac.authorization.k8s.io
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6

---
# Source: tidb-cluster/templates/discovery-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  type: ClusterIP
  ports:
  - name: discovery
    port: 10261
    targetPort: 10261
    protocol: TCP
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery

---
# Source: tidb-cluster/templates/tidb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cart-db-tidb
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  type: NodePort
  ports:
  - name: mysql-client
    port: 4000
    targetPort: 4000
    protocol: TCP
  - name: status
    port: 10080
    targetPort: 10080
    protocol: TCP
  selector:
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb

---
# Source: tidb-cluster/templates/discovery-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cart-db-discovery
  namespace: cart
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: discovery
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  # don't modify this replicas
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-cluster
      app.kubernetes.io/instance: cart-db
      app.kubernetes.io/component: discovery
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-cluster
        app.kubernetes.io/instance: cart-db
        app.kubernetes.io/component: discovery
    spec:
      serviceAccount: cart-db-discovery
      containers:
      - name: discovery
        image: pingcap/tidb-operator:v1.0.6
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi
            
        command:
          - /usr/local/bin/tidb-discovery
        env:
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace

---
# Source: tidb-cluster/templates/tidb-cluster.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: cart-db
  namespace: cart
  annotations:
    argocd.argoproj.io/hook: Skip
  labels:
    service: cart
    role: db
    app.kubernetes.io/name: tidb-cluster
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: cart-db
    app.kubernetes.io/component: tidb-cluster
    helm.sh/chart: tidb-cluster-v1.0.6
spec:
  pvReclaimPolicy: Retain
  timezone: UTC
  services:
    - name: pd
      type: ClusterIP
    
  schedulerName: tidb-scheduler
  pd:
    replicas: 3
    image: pingcap/pd:v3.0.5
    imagePullPolicy: IfNotPresent
    storageClassName: rook-ceph-block
    limits: {}
    requests:
      storage: 1Gi
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    hostNetwork: false
  tikv:
    replicas: 3
    image: pingcap/tikv:v3.0.5
    imagePullPolicy: IfNotPresent
    storageClassName: rook-ceph-block
    limits: {}
    requests:
      storage: 10Gi
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    maxFailoverCount: 3
    hostNetwork: false
  tidb:
    replicas: 0
    image: pingcap/tidb:v3.0.5
    imagePullPolicy: IfNotPresent
    limits: {}
    requests: {}
    
    affinity:
      {}
      
    nodeSelector:
      {}
      
    hostNetwork: false
    binlogEnabled: false
    maxFailoverCount: 3
    separateSlowLog: true
    slowLogTailer:
      image: busybox:1.26.2
      imagePullPolicy: IfNotPresent
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 20m
        memory: 5Mi
      

---
# Source: tidb-cluster/templates/drainer-configmap-rollout.yaml

---
# Source: tidb-cluster/templates/drainer-configmap.yaml

---
# Source: tidb-cluster/templates/drainer-service.yaml


---
# Source: tidb-cluster/templates/drainer-statefulset.yaml


---
# Source: tidb-cluster/templates/monitor-configmap.yaml


---
# Source: tidb-cluster/templates/monitor-deployment.yaml


---
# Source: tidb-cluster/templates/monitor-pvc.yaml


---
# Source: tidb-cluster/templates/monitor-rbac.yaml


---
# Source: tidb-cluster/templates/monitor-secret.yaml


---
# Source: tidb-cluster/templates/monitor-service.yaml


---
# Source: tidb-cluster/templates/pump-configmap-rollout.yaml


---
# Source: tidb-cluster/templates/pump-configmap.yaml


---
# Source: tidb-cluster/templates/pump-service.yaml


---
# Source: tidb-cluster/templates/pump-statefulset.yaml


---
# Source: tidb-cluster/templates/scheduled-backup-cronjob.yaml


---
# Source: tidb-cluster/templates/scheduled-backup-pvc.yaml


---
# Source: tidb-cluster/templates/tidb-configmap-rollout.yaml


---
# Source: tidb-cluster/templates/tidb-initializer-job.yaml


---
# Source: tidb-cluster/templates/tikv-importer-configmap.yaml


---
# Source: tidb-cluster/templates/tikv-importer-service.yaml


---
# Source: tidb-cluster/templates/tikv-importer-statefulset.yaml


